{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "file_path = 'PRSA_Data_Aotizhongxin_20130301-20170228.csv' # Or the full path\n",
        "\n",
        "try:\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"Dataset loaded successfully.\\n\")\n",
        "\n",
        "    # Combine year, month, day, hour into a single datetime column\n",
        "    df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']], errors='coerce')\n",
        "    df.set_index('datetime', inplace=True)\n",
        "    columns_to_drop = ['year', 'month', 'day', 'hour', 'No', 'station']\n",
        "    df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
        "\n",
        "    print(\"--- Initial DataFrame Head ---\")\n",
        "    print(df.head())\n",
        "    print(\"\\n--- Initial Missing Values ---\")\n",
        "    print(df.isnull().sum())\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # --- Task 2: Data Preprocessing and Feature Engineering ---\n",
        "\n",
        "    # 1. Handle Missing Values\n",
        "    print(\"--- 1. Handling Missing Values ---\")\n",
        "    # For pollutant and meteorological numerical features, linear interpolation is a common choice for time series.\n",
        "    # For PM2.5 (target), interpolation is also reasonable.\n",
        "    numerical_cols_with_na = df.select_dtypes(include=np.number).isnull().sum()\n",
        "    numerical_cols_to_interpolate = numerical_cols_with_na[numerical_cols_with_na > 0].index.tolist()\n",
        "\n",
        "    if numerical_cols_to_interpolate:\n",
        "        print(f\"Interpolating numerical columns: {numerical_cols_to_interpolate}\")\n",
        "        for col in numerical_cols_to_interpolate:\n",
        "            df[col] = df[col].interpolate(method='linear', limit_direction='both') # limit_direction fills NaNs at ends too\n",
        "    else:\n",
        "        print(\"No numerical columns found needing interpolation.\")\n",
        "\n",
        "    # For categorical 'wd' (wind direction), use forward fill then backward fill\n",
        "    if 'wd' in df.columns and df['wd'].isnull().any():\n",
        "        print(\"Filling missing 'wd' using ffill and bfill.\")\n",
        "        df['wd'] = df['wd'].fillna(method='ffill').fillna(method='bfill')\n",
        "    elif 'wd' not in df.columns:\n",
        "        print(\"Warning: 'wd' column not found.\")\n",
        "    else:\n",
        "        print(\"'wd' column has no missing values.\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- Missing Values After Handling ---\")\n",
        "    print(df.isnull().sum())\n",
        "    # If any NaNs remain (e.g., if a whole column was NaN or 'wd' had all NaNs initially),\n",
        "    # a more robust strategy might be needed, like dropping or more complex imputation.\n",
        "    # For this dataset, interpolation and ffill/bfill should handle most cases.\n",
        "    # Let's drop any rows that might still have NaNs in crucial columns (especially target)\n",
        "    # df.dropna(subset=['PM2.5'], inplace=True) # Ensure target has no NaNs\n",
        "    # print(\"\\n--- Missing Values After Final DropNA on PM2.5 (if any) ---\")\n",
        "    # print(df.isnull().sum())\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "    # 2. Create Time-Based Features\n",
        "    print(\"--- 2. Creating Time-Based Features ---\")\n",
        "    df['hour_of_day'] = df.index.hour\n",
        "    df['day_of_week'] = df.index.dayofweek  # Monday=0, Sunday=6\n",
        "    df['day_of_year'] = df.index.dayofyear\n",
        "    df['month'] = df.index.month\n",
        "    df['year'] = df.index.year # Useful for trends or splitting\n",
        "    df['week_of_year'] = df.index.isocalendar().week.astype(int)\n",
        "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
        "\n",
        "    # Season (approximation)\n",
        "    def get_season(date):\n",
        "        month = date.month\n",
        "        if month in [12, 1, 2]:\n",
        "            return 'Winter'\n",
        "        elif month in [3, 4, 5]:\n",
        "            return 'Spring'\n",
        "        elif month in [6, 7, 8]:\n",
        "            return 'Summer'\n",
        "        else: # 9, 10, 11\n",
        "            return 'Autumn'\n",
        "    df['season'] = df.index.to_series().apply(get_season)\n",
        "\n",
        "    print(\"Time-based features created:\")\n",
        "    print(df[['hour_of_day', 'day_of_week', 'month', 'season', 'is_weekend']].head())\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # 3. Handle Categorical Features ('wd' and 'season')\n",
        "    print(\"--- 3. Handling Categorical Features (One-Hot Encoding) ---\")\n",
        "    # 'wd' (wind direction) and 'season' are categorical\n",
        "    categorical_features = ['wd', 'season']\n",
        "    # Create a copy for one-hot encoding to keep original df cleaner for now\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # Check if categorical features exist before trying to encode\n",
        "    existing_categorical_features = [col for col in categorical_features if col in df_processed.columns]\n",
        "\n",
        "    if existing_categorical_features:\n",
        "        print(f\"Applying One-Hot Encoding to: {existing_categorical_features}\")\n",
        "        df_processed = pd.get_dummies(df_processed, columns=existing_categorical_features, prefix=existing_categorical_features, dummy_na=False) # dummy_na=False as we handled NaNs\n",
        "        print(\"Categorical features one-hot encoded.\")\n",
        "        print(\"DataFrame columns after one-hot encoding (sample):\")\n",
        "        print(df_processed.filter(regex='wd_|season_').head())\n",
        "    else:\n",
        "        print(\"No specified categorical features found for one-hot encoding.\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "    # 4. Analyse Correlations\n",
        "    print(\"--- 4. Analyzing Correlations ---\")\n",
        "    # Select only numerical columns for correlation matrix\n",
        "    # This includes original numerical features and new numerical time features\n",
        "    # Exclude one-hot encoded columns for the main correlation matrix for clarity,\n",
        "    # or handle them carefully if included.\n",
        "    numerical_cols_for_corr = df_processed.select_dtypes(include=np.number).columns\n",
        "\n",
        "    # It's often useful to see correlations with the target variable specifically\n",
        "    if 'PM2.5' in numerical_cols_for_corr:\n",
        "        correlation_matrix = df_processed[numerical_cols_for_corr].corr()\n",
        "        plt.figure(figsize=(18, 15)) # Adjusted size for more features\n",
        "        sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=\".2f\", linewidths=.5) # Annot=False if too cluttered\n",
        "        plt.title('Correlation Matrix of Numerical Features')\n",
        "        plt.show()\n",
        "        print(\"Displayed correlation matrix heatmap.\")\n",
        "\n",
        "        print(\"\\n--- Top Correlations with PM2.5 ---\")\n",
        "        pm25_correlations = correlation_matrix['PM2.5'].sort_values(ascending=False)\n",
        "        print(pm25_correlations)\n",
        "    else:\n",
        "        print(\"PM2.5 column not found or not numerical for correlation analysis.\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "    # 5. Normalize or Standardize Features\n",
        "    print(\"--- 5. Normalizing/Standardizing Features ---\")\n",
        "    # We will standardize numerical features. This is generally done AFTER splitting data.\n",
        "    # Here, we demonstrate how to set up the scaler.\n",
        "    # We will apply it to the `df_processed` which has one-hot encoded features.\n",
        "    # Identify numerical columns for scaling (excluding the target 'PM2.5' for now,\n",
        "    # and also excluding already binary one-hot encoded or 'is_weekend' features).\n",
        "\n",
        "    # Re-identify numerical columns from df_processed\n",
        "    numerical_features_to_scale = df_processed.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "    # Exclude target variable and binary/already scaled features if necessary\n",
        "    if 'PM2.5' in numerical_features_to_scale:\n",
        "        numerical_features_to_scale.remove('PM2.5') # Target usually not scaled with features\n",
        "\n",
        "    # Exclude one-hot encoded columns (they are already 0 or 1)\n",
        "    # and other binary features like 'is_weekend'\n",
        "    one_hot_cols = [col for col in df_processed.columns if col.startswith(tuple(f\"{cat}_\" for cat in existing_categorical_features))]\n",
        "    binary_features = ['is_weekend'] + one_hot_cols\n",
        "\n",
        "    numerical_features_to_scale = [col for col in numerical_features_to_scale if col not in binary_features]\n",
        "\n",
        "    if numerical_features_to_scale:\n",
        "        print(f\"Numerical features to be standardized: {numerical_features_to_scale}\")\n",
        "        scaler = StandardScaler()\n",
        "        # Fit and transform\n",
        "        df_processed[numerical_features_to_scale] = scaler.fit_transform(df_processed[numerical_features_to_scale])\n",
        "        print(\"Numerical features standardized.\")\n",
        "        print(df_processed[numerical_features_to_scale].head())\n",
        "    else:\n",
        "        print(\"No numerical features identified for scaling or all are binary/target.\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "    # 6. Outlier Handling (Demonstration for PM2.5)\n",
        "    print(\"--- 6. Outlier Handling Demonstration (for PM2.5) ---\")\n",
        "    if 'PM2.5' in df.columns: # Use original df for this demonstration before scaling\n",
        "        Q1 = df['PM2.5'].quantile(0.25)\n",
        "        Q3 = df['PM2.5'].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        print(f\"PM2.5 - Q1: {Q1}, Q3: {Q3}, IQR: {IQR}\")\n",
        "        print(f\"PM2.5 - Lower Bound for outliers: {lower_bound}\")\n",
        "        print(f\"PM2.5 - Upper Bound for outliers: {upper_bound}\")\n",
        "\n",
        "        outliers = df[(df['PM2.5'] < lower_bound) | (df['PM2.5'] > upper_bound)]\n",
        "        print(f\"Number of potential outliers in PM2.5: {len(outliers)}\")\n",
        "        # In a real scenario, you'd decide whether to cap, remove, or transform these,\n",
        "        # or if they are genuine extreme values to be kept.\n",
        "        # For example, capping:\n",
        "        # df_processed['PM2.5_capped'] = np.where(df_processed['PM2.5'] > upper_bound, upper_bound,\n",
        "        #                                   np.where(df_processed['PM2.5'] < lower_bound, lower_bound, df_processed['PM2.5']))\n",
        "    else:\n",
        "        print(\"PM2.5 column not found for outlier analysis.\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "    print(\"--- Final Processed DataFrame Head (df_processed) ---\")\n",
        "    print(df_processed.head())\n",
        "    print(\"\\n--- Final Processed DataFrame Info ---\")\n",
        "    df_processed.info()\n",
        "\n",
        "    print(\"\\n--- Task 2 Script Finished ---\")\n",
        "\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{file_path}' was not found.\")\n",
        "    print(\"Please ensure the file path is correct and the CSV file is in the specified location.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    print(\"Please check your data and script carefully.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The file 'PRSA_Data_Aotizhongxin_20130301-20170228.csv' was not found.\n",
            "Please ensure the file path is correct and the CSV file is in the specified location.\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "J-WIrQoR4x46",
        "outputId": "ce3b5cf8-ae68-4d59-87a6-dc86ebe5e994",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}